#__init__.py

# sales_prediction_project/__init__.py
# This file can be empty, or you can add package-level initialization code.
# It can be left empty or used for package-level imports or settings.

# Example: Importing key functions directly at the package level
from .validation import validate_columns_and_values
from .cleaning import clean_data
from .feature_engineering import feature_engineering
from .forecasting import prepare_dataset, train_prophet_model, generate_forecast, auto_detect_columns

















#app.py

import streamlit as st
import pandas as pd
import numpy as np
from prophet import Prophet
import plotly.express as px
from metrics import mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error
from forecasting import (
    train_prophet_model, train_arima_model, train_sarima_model, train_xgb_model,
    prepare_dataset, get_train_test_split, auto_detect_columns
)
from auth import login_form
from firebase_auth import google_login, email_password_login


def main():
    st.title("📊 ProCode Sales Prediction Platform")

    if "user" not in st.session_state:
        st.session_state["user"] = None

    if not st.session_state["user"]:
        st.info("Please sign in to continue")
        tab1, tab2 = st.tabs(["Username/Password", "Google Login"])
        with tab1:
            login_form()
        with tab2:
            google_login()
        return  # stop here until logged in

    # 🚀 If logged in → show your existing workflow
    st.success(f"Welcome {st.session_state['user']}!")
    st.write("Now you can access the full workflow here...")


    if st.button("Logout"):
        st.session_state["user"] = None
        st.rerun()



st.set_page_config(page_title="Forecast Dashboard", layout="wide")
st.title("📈 Procode Modeling Platform")

# Sidebar navigation
section = st.sidebar.radio("Go to", [
    "Upload", "Raw Data", "Cleaned Data", "Feature Engineered Data",
    "Forecast Table", "Forecast Chart", "YoY Growth", "Summary Dashboard",
    "Model Explainability", "Advanced Diagnostics", "Download Reports"
])

# Sidebar: Model selection and hyperparameters
st.sidebar.header("Forecast Settings")
model_choice = st.sidebar.selectbox(
    "Select Model", ["Auto (Best)", "Prophet", "ARIMA", "SARIMA", "XGBoost"], index=0
)
seasonality_mode = st.sidebar.selectbox("Seasonality Mode", ["additive", "multiplicative"], index=0)
changepoint_prior_scale = st.sidebar.slider("Changepoint Prior Scale", 0.001, 0.5, 0.05)
arima_p = st.sidebar.number_input("ARIMA p (AR)", min_value=0, max_value=5, value=1)
arima_d = st.sidebar.number_input("ARIMA d (I)", min_value=0, max_value=2, value=1)
arima_q = st.sidebar.number_input("ARIMA q (MA)", min_value=0, max_value=5, value=1)
sarima_p = st.sidebar.number_input("SARIMA p (AR)", min_value=0, max_value=5, value=1)
sarima_d = st.sidebar.number_input("SARIMA d (I)", min_value=0, max_value=2, value=1)
sarima_q = st.sidebar.number_input("SARIMA q (MA)", min_value=0, max_value=5, value=1)
sarima_P = st.sidebar.number_input("SARIMA P (seasonal AR)", min_value=0, max_value=5, value=1)
sarima_D = st.sidebar.number_input("SARIMA D (seasonal I)", min_value=0, max_value=2, value=1)
sarima_Q = st.sidebar.number_input("SARIMA Q (seasonal MA)", min_value=0, max_value=5, value=1)
sarima_s = st.sidebar.number_input("SARIMA s (seasonal period)", min_value=1, max_value=365, value=12)

if 'ran_forecast' not in st.session_state:
    st.session_state["ran_forecast"] = False

# ------------------- Upload Section & Rest of App -------------------

if section == "Upload":
    uploaded_file = st.file_uploader("Upload your dataset (CSV or Excel)", type=["csv", "xlsx"])
    if uploaded_file:
        if uploaded_file.name.endswith('.csv'):
            data = pd.read_csv(uploaded_file)
        else:
            data = pd.read_excel(uploaded_file)
        st.session_state["raw_data"] = data.copy()

        # Simple cleaning: drop duplicates, basic fillna
        cleaned_data = data.drop_duplicates().copy()
        for col in cleaned_data.select_dtypes(include='object').columns:
            cleaned_data[col] = cleaned_data[col].replace(["Not Available", "NULL", "Missing", "Unknown", " "], np.nan)
        for col in cleaned_data.columns:
            if cleaned_data[col].isnull().sum() > 0:
                if cleaned_data[col].dtype in ['float64', 'int64']:
                    cleaned_data[col].fillna(cleaned_data[col].median(), inplace=True)
                elif pd.api.types.is_datetime64_any_dtype(cleaned_data[col]):
                    cleaned_data[col].fillna(cleaned_data[col].mode()[0], inplace=True)
                elif cleaned_data[col].dtype == 'object':
                    cleaned_data[col].fillna("Unknown", inplace=True)
        st.session_state["cleaned_data"] = cleaned_data.copy()

        # Feature engineering (basic: add year/month/day columns if date present)
        fe_data = cleaned_data.copy()
        date_cols = [col for col in fe_data.columns if pd.api.types.is_datetime64_any_dtype(fe_data[col]) or 'date' in col.lower()]
        for col in date_cols:
            fe_data[col] = pd.to_datetime(fe_data[col], errors='coerce')
            fe_data[f"{col}_year"] = fe_data[col].dt.year
            fe_data[f"{col}_month"] = fe_data[col].dt.month
            fe_data[f"{col}_day"] = fe_data[col].dt.day
            fe_data[f"{col}_dayofweek"] = fe_data[col].dt.dayofweek
        st.session_state["fe_data"] = fe_data.copy()

        st.success("Data uploaded and processed. Please proceed to individual sections for detailed view or run forecast.")

        st.subheader("📄 Raw Data Preview")
        st.dataframe(data.head())

        st.subheader("🧼 Cleaned Data Preview")
        st.dataframe(cleaned_data.head())

        st.subheader("🛠️ Feature Engineered Data Preview")
        st.dataframe(fe_data.head())

        # Column selection for forecasting
        auto_date_col, auto_target_col = auto_detect_columns(fe_data)
        date_col = st.selectbox("Select Date Column (ds)", fe_data.columns, index=fe_data.columns.get_loc(auto_date_col))
        num_cols = fe_data.select_dtypes(include=np.number).columns
        target_col = st.selectbox("Select Target Column (y)", num_cols, index=list(num_cols).index(auto_target_col))

        st.session_state['date_col'] = date_col
        st.session_state['target_col'] = target_col
        st.session_state['fe_data'] = fe_data

        if st.button("Run Forecast"):
            forecast_data = prepare_dataset(fe_data, date_col, target_col)
            selected_model = model_choice
            forecast = None
            model = None

            if selected_model == "Auto (Best)":
                st.info("Auto-selecting best model using last 30 points as test set...")
                with st.spinner("Comparing models, please wait..."):
                    train, test = get_train_test_split(forecast_data, test_size=30)
                    models_to_try = [
                        ("Prophet", lambda: train_prophet_model(
                            train, seasonality_mode=seasonality_mode, changepoint_prior_scale=changepoint_prior_scale,
                            periods=len(test), return_model=True)),
                        ("ARIMA", lambda: train_arima_model(
                            train, order=(arima_p, arima_d, arima_q),
                            forecast_periods=len(test), return_model=True)),
                        ("SARIMA", lambda: train_sarima_model(
                            train, order=(sarima_p, sarima_d, sarima_q),
                            seasonal_order=(sarima_P, sarima_D, sarima_Q, sarima_s),
                            forecast_periods=len(test), return_model=True)),
                        ("XGBoost", lambda: train_xgb_model(
                            train, forecast_periods=len(test), return_model=True))
                    ]
                    best_model_name = None
                    best_rmse = np.inf
                    best_model_obj = None
                    best_forecast = None
                    model_results = []
                    for model_name, model_func in models_to_try:
                        try:
                            fc, m_obj = model_func()
                            y_true = test['y'].values
                            if fc.shape[0] > len(y_true):
                                y_pred = fc['yhat'].values[-len(y_true):]
                            else:
                                y_pred = fc['yhat'].values
                            rmse = root_mean_squared_error(y_true, y_pred)
                            model_results.append((model_name, rmse, fc))
                            if rmse < best_rmse:
                                best_rmse = rmse
                                best_model_name = model_name
                                best_model_obj = m_obj
                                best_forecast = fc
                        except Exception as e:
                            st.warning(f"Model {model_name} failed: {e}")

                    st.subheader("Model RMSE Comparison")
                    st.table({name: round(rmse, 2) for name, rmse, _ in model_results})
                    st.success(f"Best model: {best_model_name} (RMSE={best_rmse:.2f}). Running on full dataset...")

                with st.spinner(f"Generating final forecast with {best_model_name}..."):
                    if best_model_name == "Prophet":
                        forecast, model = train_prophet_model(
                            forecast_data,
                            seasonality_mode=seasonality_mode,
                            changepoint_prior_scale=changepoint_prior_scale,
                            periods=365,
                            return_model=True)
                    elif best_model_name == "ARIMA":
                        forecast, model = train_arima_model(
                            forecast_data,
                            order=(arima_p, arima_d, arima_q),
                            forecast_periods=365,
                            return_model=True)
                    elif best_model_name == "SARIMA":
                        forecast, model = train_sarima_model(
                            forecast_data,
                            order=(sarima_p, sarima_d, sarima_q),
                            seasonal_order=(sarima_P, sarima_D, sarima_Q, sarima_s),
                            forecast_periods=365,
                            return_model=True)
                    elif best_model_name == "XGBoost":
                        forecast, model = train_xgb_model(
                            forecast_data,
                            forecast_periods=365,
                            return_model=True)
                    selected_model = best_model_name

            else:
                with st.spinner(f"Running {selected_model} model..."):
                    if selected_model == "Prophet":
                        forecast, model = train_prophet_model(
                            forecast_data,
                            seasonality_mode=seasonality_mode,
                            changepoint_prior_scale=changepoint_prior_scale,
                            periods=365,
                            return_model=True
                        )
                    elif selected_model == "ARIMA":
                        order = (arima_p, arima_d, arima_q)
                        forecast, model = train_arima_model(
                            forecast_data,
                            order=order,
                            forecast_periods=365,
                            return_model=True)
                    elif selected_model == "SARIMA":
                        order = (sarima_p, sarima_d, sarima_q)
                        seasonal_order = (sarima_P, sarima_D, sarima_Q, sarima_s)
                        forecast, model = train_sarima_model(
                            forecast_data,
                            order=order,
                            seasonal_order=seasonal_order,
                            forecast_periods=365,
                            return_model=True)
                    elif selected_model == "XGBoost":
                        forecast, model = train_xgb_model(
                            forecast_data,
                            forecast_periods=365,
                            return_model=True)

            st.session_state["forecast"] = forecast
            st.session_state["ran_forecast"] = True
            st.session_state["date_col"] = date_col
            st.session_state["target_col"] = target_col
            st.session_state["selected_model"] = selected_model
            st.session_state["model"] = model

            if forecast is not None:
                st.subheader(f"Forecast Table ({selected_model})")
                st.dataframe(forecast[["ds", "yhat", "yhat_lower", "yhat_upper"]])

                st.subheader(f"Forecast Chart ({selected_model})")
                fig = px.line(forecast, x="ds", y="yhat", title=f"Forecasted Values for {target_col} ({selected_model})")
                fig.add_scatter(x=forecast["ds"], y=forecast["yhat_upper"], mode='lines', name='Upper Bound')
                fig.add_scatter(x=forecast["ds"], y=forecast["yhat_lower"], mode='lines', name='Lower Bound')
                st.plotly_chart(fig, use_container_width=True)

                st.subheader(f"Year-over-Year Growth ({selected_model})")
                forecast["year"] = forecast["ds"].dt.year
                yearly = forecast.groupby("year")["yhat"].sum().reset_index()
                yearly["YoY Growth %"] = yearly["yhat"].pct_change() * 100
                fig2 = px.line(yearly, x="year", y="yhat", title="Total Forecast by Year")
                st.plotly_chart(fig2, use_container_width=True)

                st.markdown("---")
                st.info(f"✅ Forecast complete using **{selected_model}**! Please visit the navigation panel for individual graph analysis.")
            else:
                st.error("Forecasting failed for all models. Please check your data or model settings.")


# Add your sections for Raw Data, Cleaned Data, Feature Engineered Data, etc. (as before).
# === Data preview sections ===

if section == "Raw Data" and "raw_data" in st.session_state:
    st.subheader("📄 Raw Data")
    st.dataframe(st.session_state["raw_data"].head())

if section == "Cleaned Data" and "cleaned_data" in st.session_state:
    st.subheader("🧼 Cleaned Data")
    st.dataframe(st.session_state["cleaned_data"].head())

if section == "Feature Engineered Data" and "fe_data" in st.session_state:
    st.subheader("🛠️ Feature Engineered Data")
    st.write("Feature engineered data includes additional columns like year, month, day, and day of week extracted from date columns.")
    st.dataframe(st.session_state["fe_data"].head())

# === Results sections ===

if section in ["Forecast Table", "Forecast Chart", "YoY Growth", "Summary Dashboard"] and not st.session_state.get("ran_forecast"):
    st.warning("Please upload data and run a forecast first.")

if section == "Forecast Table" and st.session_state.get("ran_forecast"):
    forecast = st.session_state["forecast"]
    st.subheader("📋 Forecast Table")
    st.write("Numerical forecast results with confidence intervals.")
    st.dataframe(forecast[["ds", "yhat", "yhat_lower", "yhat_upper"]])

if section == "Forecast Chart" and st.session_state.get("ran_forecast"):
    forecast = st.session_state["forecast"]
    st.subheader("📈 Forecast Visualization")
    model_used = st.session_state.get("selected_model", "Prophet")
    st.info(f"Model Used: {model_used}")
    fig = px.line(forecast, x="ds", y="yhat", title=f"Forecasted Values ({model_used})")
    fig.add_scatter(x=forecast["ds"], y=forecast["yhat_upper"], mode='lines', name='Upper Bound')
    fig.add_scatter(x=forecast["ds"], y=forecast["yhat_lower"], mode='lines', name='Lower Bound')
    st.plotly_chart(fig, use_container_width=True)

if section == "YoY Growth" and st.session_state.get("ran_forecast"):
    forecast = st.session_state["forecast"]
    forecast["year"] = forecast["ds"].dt.year
    yearly = forecast.groupby("year")["yhat"].sum().reset_index()
    yearly["YoY Growth %"] = yearly["yhat"].pct_change() * 100
    st.subheader("📊 Year-over-Year Growth Analysis")
    st.dataframe(yearly)
    fig = px.bar(yearly, x="year", y="YoY Growth %", title="Year-over-Year Growth %")
    st.plotly_chart(fig, use_container_width=True)

if section == "Summary Dashboard" and st.session_state.get("ran_forecast"):
    st.subheader("📊 Summary Dashboard")
    model_used = st.session_state.get("selected_model", "Prophet")
    st.info(f"Model Used: {model_used}")
    forecast = st.session_state["forecast"]
    latest_forecast = forecast["yhat"].iloc[-1]
    total_forecast = forecast["yhat"].sum()
    yearly = forecast.copy()
    yearly["year"] = yearly["ds"].dt.year
    yoy_df = yearly.groupby("year")["yhat"].sum().pct_change().reset_index()
    avg_yoy = yoy_df["yhat"].mean() * 100

    col1, col2, col3 = st.columns(3)
    col1.metric("Latest Forecast Value", f"{latest_forecast:,.2f}")
    col2.metric("Total Forecast Value", f"{total_forecast:,.2f}")
    col3.metric("Average YoY Growth", f"{avg_yoy:.2f}%")

# === Model Explainability ===

if section == "Model Explainability" and st.session_state.get("ran_forecast"):
    st.subheader("🔍 Model Explainability & Insights")
    model = st.session_state.get("model", None)
    model_used = st.session_state.get("selected_model", None)
    forecast = st.session_state.get("forecast", None)
    if model_used == "Prophet" and model is not None:
        st.write("**Prophet Trend and Seasonality Components**")
        st.pyplot(model.plot_components(forecast))
    elif model_used == "XGBoost" and model is not None:
        st.write("**XGBoost Feature Importance**")
        importances = model.feature_importances_
        feature_names = model.get_booster().feature_names
        fi_df = pd.DataFrame({"Feature": feature_names, "Importance": importances})
        fi_df = fi_df.sort_values("Importance", ascending=False)
        st.bar_chart(fi_df.set_index("Feature"))
        try:
            import shap
            fe_data = st.session_state["fe_data"]
            date_col = st.session_state["date_col"]
            target_col = st.session_state["target_col"]
            df = fe_data.copy()
            df['year'] = df[date_col].dt.year
            df['month'] = df[date_col].dt.month
            df['day'] = df[date_col].dt.day
            df['dayofweek'] = df[date_col].dt.dayofweek
            df['y_lag1'] = df[target_col].shift(1)
            df = df.dropna()
            X = df[['year', 'month', 'day', 'dayofweek', 'y_lag1']]
            explainer = shap.Explainer(model)
            shap_values = explainer(X)
            st.set_option('deprecation.showPyplotGlobalUse', False)
            st.write("**XGBoost SHAP Feature Impact:**")
            st.pyplot(shap.summary_plot(shap_values, X, show=False))
        except Exception as e:
            st.info(f"SHAP summary not available: {e}")
    elif model_used in ["ARIMA", "SARIMA"] and model is not None:
        st.write("**Residual Diagnostics**")
        try:
            import matplotlib.pyplot as plt
            from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
            fe_data = st.session_state["fe_data"]
            date_col = st.session_state["date_col"]
            target_col = st.session_state["target_col"]
            actual = fe_data.set_index(date_col)[target_col]
            pred = forecast.set_index("ds")["yhat"]
            actual = actual.loc[actual.index.intersection(pred.index)]
            pred = pred.loc[actual.index]
            residuals = actual - pred
            fig, ax = plt.subplots(2, 2, figsize=(12, 8))
            ax[0,0].plot(residuals.index, residuals.values)
            ax[0,0].set_title("Residuals Over Time")
            ax[0,1].hist(residuals.dropna(), bins=30)
            ax[0,1].set_title("Residuals Histogram")
            plot_acf(residuals.dropna(), ax=ax[1,0], lags=40)
            ax[1,0].set_title("Residuals Autocorrelation")
            plot_pacf(residuals.dropna(), ax=ax[1,1], lags=40)
            ax[1,1].set_title("Residuals Partial Autocorrelation")
            st.pyplot(fig)
            st.caption("Ideally, residuals should look random and centered on zero.")
        except Exception as e:
            st.info(f"Residual diagnostics not available: {e}")
    else:
        st.warning("Model explainability is only available after running a forecast with Prophet, XGBoost, ARIMA, or SARIMA. Please rerun forecast.")

# === Advanced Diagnostics (Prophet Cross-validation, etc.) ===

if section == "Advanced Diagnostics" and st.session_state.get("ran_forecast"):
    st.subheader("🧪 Advanced Diagnostics & Model Quality")
    model_used = st.session_state.get("selected_model", "Prophet")
    model = st.session_state.get("model", None)
    forecast = st.session_state.get("forecast", None)
    if model_used == "Prophet" and model is not None:
        from prophet.diagnostics import cross_validation, performance_metrics
        try:
            with st.spinner("Running Prophet cross-validation..."):
                df_cv = cross_validation(model, initial="365 days", period="180 days", horizon="90 days", parallel="processes")
                df_p = performance_metrics(df_cv)
            st.dataframe(df_p[["horizon", "mse", "rmse", "mae", "mape"]])
            st.line_chart(df_p.set_index("horizon")[["rmse", "mae"]])
        except Exception as e:
            st.warning(f"Prophet cross-validation not available: {e}")

# === Compare Model Errors (manual benchmarking) ===

if st.session_state.get("ran_forecast"):
    st.markdown("---")
    st.subheader("Compare Model Errors (Last 30 Actual Points)")

    if st.button("Compare All Models"):
        fe_data = st.session_state.get("fe_data")
        date_col = st.session_state.get("date_col")
        target_col = st.session_state.get("target_col")
        forecast_data = prepare_dataset(fe_data, date_col, target_col)
        train, test = get_train_test_split(forecast_data, test_size=30)
        models_to_try = [
            ("Prophet", lambda: train_prophet_model(
                train, seasonality_mode=seasonality_mode, changepoint_prior_scale=changepoint_prior_scale,
                periods=len(test), return_model=True)),
            ("ARIMA", lambda: train_arima_model(
                train, order=(arima_p, arima_d, arima_q),
                forecast_periods=len(test), return_model=True)),
            ("SARIMA", lambda: train_sarima_model(
                train, order=(sarima_p, sarima_d, sarima_q),
                seasonal_order=(sarima_P, sarima_D, sarima_Q, sarima_s),
                forecast_periods=len(test), return_model=True)),
            ("XGBoost", lambda: train_xgb_model(
                train, forecast_periods=len(test), return_model=True))
        ]
        results = []
        for model_name, model_func in models_to_try:
            try:
                fc, m_obj = model_func()
                y_true = test['y'].values
                if fc.shape[0] > len(y_true):
                    y_pred = fc['yhat'].values[-len(y_true):]
                else:
                    y_pred = fc['yhat'].values
                mae = mean_absolute_error(y_true, y_pred)
                rmse = root_mean_squared_error(y_true, y_pred)
                mape = mean_absolute_percentage_error(y_true, y_pred)
                results.append((model_name, mae, rmse, mape))
            except Exception as e:
                st.warning(f"Model {model_name} failed: {e}")
        if results:
            results_df = pd.DataFrame(results, columns=["Model", "MAE", "RMSE", "MAPE"])
            results_df["MAE"] = results_df["MAE"].round(2)
            results_df["RMSE"] = results_df["RMSE"].round(2)
            results_df["MAPE"] = results_df["MAPE"].round(2)
            st.table(results_df.set_index("Model"))


if section == "Download Reports" and st.session_state.get("ran_forecast"):
    st.subheader("📥 Download Reports")
    st.write("""
    Download cleaned, feature engineered, and forecasted data as CSV.
    """)

    # Download Cleaned Data
    try:
        cleaned_csv = st.session_state["cleaned_data"].to_csv(index=False).encode('utf-8')
        st.download_button(
            label="📄 Download Cleaned Data (CSV)",
            data=cleaned_csv,
            file_name="cleaned_data.csv",
            mime="text/csv"
        )
    except Exception as e:
        st.error(f"Error preparing Cleaned Data CSV: {e}")

    # Download Feature Engineered Data
    try:
        fe_csv = st.session_state["fe_data"].to_csv(index=False).encode('utf-8')
        st.download_button(
            label="📄 Download Feature Engineered Data (CSV)",
            data=fe_csv,
            file_name="feature_engineered_data.csv",
            mime="text/csv"
        )
    except Exception as e:
        st.error(f"Error preparing Feature Engineered Data CSV: {e}")

    # Download Forecasted Results
    try:
        forecast = st.session_state["forecast"]
        slim_forecast = forecast[["ds", "yhat", "yhat_lower", "yhat_upper"]].copy()
        forecast_csv = slim_forecast.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="📄 Download Forecast Results (CSV)",
            data=forecast_csv,
            file_name="forecast_results.csv",
            mime="text/csv"
        )
    except Exception as e:
        st.error(f"Error preparing Forecast Results CSV: {e}")


if section == "Summary Dashboard" and st.session_state.get("ran_forecast"):
    st.markdown("---")
    feedback = st.text_area("Any feedback, issues, or suggestions?")
    if st.button("Submit Feedback"):
        if feedback.strip():
            st.success("Thank you for your feedback!")
        else:
            st.warning("Please enter some feedback before submitting.")


if __name__ == "__main__":
    main()


















#cleaning.py

import pandas as pd
import numpy as np

def clean_data(data):
    data.columns = [col.strip().lower().replace(" ", "_") for col in data.columns]
    for column in data.columns:
        if data[column].dtype == 'object':
            data[column] = data[column].replace(["Not Available", "NULL", "Missing", "Unknown", " "], np.nan)
            if any(kw in column for kw in ['date', 'time']):
                try:
                    temp_series = pd.to_datetime(data[column], errors='coerce')
                    if temp_series.notna().mean() > 0.5:
                        data[column] = temp_series
                except:
                    pass
    for column in data.columns:
        if data[column].isnull().sum() > 0:
            if data[column].dtype in ['float64', 'int64']:
                data[column] = pd.to_numeric(data[column], errors='coerce')
                data[column].fillna(data[column].median(), inplace=True)
            elif pd.api.types.is_datetime64_any_dtype(data[column]):
                data[column].fillna(data[column].mode()[0], inplace=True)
            elif data[column].dtype == 'object':
                data[column].fillna("Unknown", inplace=True)
    data = data.drop_duplicates()
    return data














#feature_engineering

import pandas as pd
import streamlit as st

def feature_engineering(data):
    preserved_columns = ['ds', 'y']
    data.columns = [col.strip().lower().replace(" ", "_") for col in data.columns]
    date_columns = [col for col in data.columns if pd.api.types.is_datetime64_any_dtype(data[col])]

    for date_col in date_columns:
        try:
            data[f"{date_col}_year"] = data[date_col].dt.year
            data[f"{date_col}_month"] = data[date_col].dt.month
            data[f"{date_col}_day"] = data[date_col].dt.day
            data[f"{date_col}_dayofweek"] = data[date_col].dt.dayofweek
        except Exception as e:
            st.warning(f"Date processing warning in column '{date_col}': {e}")

    return data














#forecasting

from prophet import Prophet
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
import xgboost as xgb
import pandas as pd

def train_prophet_model(data, seasonality_mode='additive', changepoint_prior_scale=0.05, periods=30, return_model=False):
    df = data.copy()
    df = df.rename(columns={"ds": "ds", "y": "y"})
    model = Prophet(seasonality_mode=seasonality_mode, changepoint_prior_scale=changepoint_prior_scale)
    model.fit(df)
    future = model.make_future_dataframe(periods=periods)
    forecast = model.predict(future)
    if return_model:
        return (forecast, model)
    else:
        return forecast

def train_arima_model(data, order=(1,1,1), forecast_periods=365, return_model=False):
    df = data.copy()
    df = df.groupby('ds', as_index=False).agg({'y': 'sum'})
    df = df.set_index('ds').asfreq('D')
    df['y'].interpolate(inplace=True)
    model = ARIMA(df['y'], order=order)
    model_fit = model.fit()
    forecast = model_fit.forecast(steps=forecast_periods)
    forecast_index = pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=forecast_periods, freq='D')
    forecast_df = pd.DataFrame({'ds': forecast_index, 'yhat': forecast.values})
    forecast_df['yhat_lower'] = forecast_df['yhat'] * 0.95
    forecast_df['yhat_upper'] = forecast_df['yhat'] * 1.05
    if return_model:
        return (forecast_df, model_fit)
    else:
        return forecast_df

def train_sarima_model(data, order=(1,1,1), seasonal_order=(1,1,1,12), forecast_periods=365, return_model=False):
    df = data.copy()
    df = df.groupby('ds', as_index=False).agg({'y': 'sum'})
    df = df.set_index('ds').asfreq('D')
    df['y'].interpolate(inplace=True)
    model = SARIMAX(df['y'], order=order, seasonal_order=seasonal_order)
    model_fit = model.fit(disp=False)
    forecast = model_fit.forecast(steps=forecast_periods)
    forecast_index = pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=forecast_periods, freq='D')
    forecast_df = pd.DataFrame({'ds': forecast_index, 'yhat': forecast.values})
    forecast_df['yhat_lower'] = forecast_df['yhat'] * 0.95
    forecast_df['yhat_upper'] = forecast_df['yhat'] * 1.05
    if return_model:
        return (forecast_df, model_fit)
    else:
        return forecast_df

def train_xgb_model(data, forecast_periods=365, return_model=False):
    df = data.copy()
    df = df.sort_values("ds")
    df['year'] = df['ds'].dt.year
    df['month'] = df['ds'].dt.month
    df['day'] = df['ds'].dt.day
    df['dayofweek'] = df['ds'].dt.dayofweek
    df['y_lag1'] = df['y'].shift(1)
    df = df.dropna()
    X = df[['year', 'month', 'day', 'dayofweek', 'y_lag1']]
    y = df['y']

    model = xgb.XGBRegressor()
    model.fit(X, y)

    last_row = df.iloc[-1].copy()
    future_rows = []
    for i in range(forecast_periods):
        next_date = last_row['ds'] + pd.Timedelta(days=1)
        features = {
            'year': next_date.year,
            'month': next_date.month,
            'day': next_date.day,
            'dayofweek': next_date.dayofweek,
            'y_lag1': last_row['y']
        }
        X_pred = pd.DataFrame([features])
        y_pred = model.predict(X_pred)[0]
        future_rows.append({'ds': next_date, 'yhat': y_pred})
        last_row = pd.Series({'ds': next_date, 'y': y_pred, **features})

    forecast_df = pd.DataFrame(future_rows)
    forecast_df['yhat_lower'] = forecast_df['yhat'] * 0.95
    forecast_df['yhat_upper'] = forecast_df['yhat'] * 1.05
    if return_model:
        return (forecast_df, model)
    else:
        return forecast_df

def prepare_dataset(df, date_col, target_col):
    df = df[[date_col, target_col]].copy()
    df = df.rename(columns={date_col: "ds", target_col: "y"})
    df = df.sort_values("ds")
    df = df.dropna()
    df["ds"] = pd.to_datetime(df["ds"])
    df = df[["ds", "y"]]
    return df

def get_train_test_split(data, test_size=30):
    train = data.iloc[:-test_size]
    test = data.iloc[-test_size:]
    return train, test

def auto_detect_columns(df):
    date_cols = [col for col in df.columns if pd.api.types.is_datetime64_any_dtype(df[col]) or 'date' in col.lower()]
    num_cols = df.select_dtypes(include='number').columns
    date_col = date_cols[0] if date_cols else df.columns[0]
    target_col = num_cols[0] if len(num_cols) else df.columns[-1]
    return date_col, target_col












#pdf_gen

import io
from reportlab.platypus import SimpleDocTemplate, Paragraph, Table, TableStyle, Spacer, Image
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.pagesizes import letter
from reportlab.lib import colors



def generate_pdf_report(forecast, metrics, forecast_image_path, yoy_image_path, cleaned_data=None, fe_data=None, date_col=None, target_col=None):
    buffer = io.BytesIO()
    pdf = SimpleDocTemplate(buffer, pagesize=letter)
    styles = getSampleStyleSheet()
    elements = []

    # Title
    elements.append(Paragraph(f"Forecast Report", styles['Title']))
    elements.append(Spacer(1, 12))

    # Summary Metrics
    summary_text = f"""
    <b>Key Metrics:</b><br/>
    - Total Forecast Value: {metrics['total_forecast']:,.2f}<br/>
    - Latest Forecast Value: {metrics['latest_forecast']:,.2f}<br/>
    - Average YoY Growth: {metrics['average_yoy']:.2f}%
    """
    elements.append(Paragraph(summary_text, styles['BodyText']))
    elements.append(Spacer(1, 12))

    # Forecast Graph
    elements.append(Paragraph("<b>Forecast Graph:</b>", styles['Heading2']))
    elements.append(Image(forecast_image_path, width=400, height=200))
    elements.append(Spacer(1, 12))

    # YoY Growth Graph
    elements.append(Paragraph("<b>Year-over-Year Growth:</b>", styles['Heading2']))
    elements.append(Image(yoy_image_path, width=400, height=200))
    elements.append(Spacer(1, 12))

    # Forecast Table
    elements.append(Paragraph("<b>Forecast Table:</b>", styles['Heading2']))
    table_data = forecast[["ds", "yhat", "yhat_lower", "yhat_upper"]].head(10).values.tolist()
    table_data.insert(0, ["Date", "Forecast Value", "Lower Bound", "Upper Bound"])
    table = Table(table_data, colWidths=[100, 100, 100, 100])
    table.setStyle(TableStyle([
        ("GRID", (0, 0), (-1, -1), 0.5, colors.black),
        ("BACKGROUND", (0, 0), (-1, 0), colors.grey),
        ("FONT", (0, 0), (-1, 0), "Helvetica-Bold"),
        ("ALIGN", (0, 0), (-1, -1), "CENTER"),
    ]))
    elements.append(table)

    pdf.build(elements)
    buffer.seek(0)
    return buffer







# validation.py

import numpy as np
import pandas as pd

# Validation Framework
def validate_columns_and_values(data):
    validation_results = {}
    data.columns = [col.strip().lower().replace(" ", "_") for col in data.columns]

    for column in data.columns:
        validation_results[column] = {
            "missing_count": 0,
            "missing_values": [],
            "invalid_count": 0,
            "invalid_values": [],
            "outlier_count": 0,
            "outlier_values": []
        }

        if data[column].dtype in ['float64', 'int64']:
            missing = data[data[column].isnull()]
            validation_results[column]["missing_count"] = missing.shape[0]
            validation_results[column]["missing_values"] = list(missing[column].unique())
            invalid = data[data[column] < 0]
            validation_results[column]["invalid_count"] = invalid.shape[0]
            validation_results[column]["invalid_values"] = list(invalid[column].unique())
            data[column] = data[column].apply(lambda x: max(x, 0))
            z_scores = (data[column] - data[column].mean()) / data[column].std()
            outliers = data[np.abs(z_scores) > 3]
            validation_results[column]["outlier_count"] = outliers.shape[0]
            validation_results[column]["outlier_values"] = list(outliers[column].unique())

        elif data[column].dtype == 'object':
            missing = data[data[column].isnull() | (data[column] == "")]
            validation_results[column]["missing_count"] = missing.shape[0]
            validation_results[column]["missing_values"] = list(missing[column].unique())
            invalid = data[data[column].str.contains(r"NULL|Not Available|Missing", na=False)]
            validation_results[column]["invalid_count"] = invalid.shape[0]
            validation_results[column]["invalid_values"] = list(invalid[column].unique())

        elif pd.api.types.is_datetime64_any_dtype(data[column]):
            missing = data[data[column].isnull()]
            validation_results[column]["missing_count"] = missing.shape[0]
            validation_results[column]["missing_values"] = list(missing[column].unique())
            invalid = data[data[column] > pd.Timestamp.now()]
            validation_results[column]["invalid_count"] = invalid.shape[0]
            validation_results[column]["invalid_values"] = list(invalid[column].unique())

    return validation_results









    #requirements.text

    absl-py==2.2.2
aiohappyeyeballs==2.6.1
aiohttp==3.11.18
aiosignal==1.3.2
altair==5.5.0
attrs==25.3.0
beautifulsoup4==4.13.4
blinker==1.9.0
CacheControl==0.14.2
cachetools==5.5.2
certifi==2025.4.26
cffi==1.17.1
chardet==5.2.0
charset-normalizer==3.4.1
click==8.1.8
cmdstanpy==1.2.5
colorama==0.4.6
contourpy==1.3.2
cryptography==44.0.2
cycler==0.12.1
et_xmlfile==2.0.0
filelock==3.18.0
firebase-admin==6.8.0
fonttools==4.57.0
frozenlist==1.6.0
fsspec==2025.3.2
gitdb==4.0.12
GitPython==3.1.44
google==3.0.0
google-api-core==2.25.0rc0
google-api-python-client==2.168.0
google-auth==2.39.0
google-auth-httplib2==0.2.0
google-cloud-core==2.4.3
google-cloud-firestore==2.20.2
google-cloud-storage==3.1.0
google-crc32c==1.7.1
google-resumable-media==2.7.2
googleapis-common-protos==1.70.0
grpcio==1.71.0
grpcio-status==1.71.0
holidays==0.71
httplib2==0.22.0
idna==3.10
importlib_resources==6.5.2
Jinja2==3.1.6
joblib==1.4.2
jsonschema==4.23.0
jsonschema-specifications==2025.4.1
kaleido==0.2.1
kiwisolver==1.4.8
lightning-utilities==0.14.3
Markdown==3.8
MarkupSafe==3.0.2
matplotlib==3.10.1
msgpack==1.1.0
multidict==6.4.3
narwhals==1.36.0
networkx==3.4.2
numpy==2.2.5
openpyxl==3.1.5
packaging==24.2
pandas==2.2.3
pillow==11.2.1
platformdirs==4.3.7
plotly==5.24.1
propcache==0.3.1
prophet==1.1.6
proto-plus==1.26.1
protobuf==5.29.4
pyarrow==19.0.1
pyasn1==0.6.1
pyasn1_modules==0.4.2
pycparser==2.22
pycryptodome==3.22.0
pydeck==0.9.1
PyJWT==2.10.1
pyparsing==3.2.3
Pyrebase4==4.8.0
python-dateutil==2.9.0.post0
python-jwt==2.0.1
pytorch-lightning==1.9.5
PyYAML==6.0.2
referencing==0.36.2
reportlab==4.4.0
requests==2.32.3
requests-toolbelt==0.9.1
rpds-py==0.24.0
rsa==4.9
scikit-learn==1.6.1
scipy==1.15.2
seaborn==0.13.2
setuptools==78.1.0
six==1.17.0
smmap==5.0.2
soupsieve==2.7
stanio==0.5.1
streamlit==1.44.1
sympy==1.13.1
tenacity==9.1.2
tensorboard==2.19.0
tensorboard-data-server==0.7.2
threadpoolctl==3.6.0
toml==0.10.2
torch==2.6.0
torchaudio==2.6.0
torchmetrics==1.7.0
torchvision==0.21.0
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
typing_extensions==4.13.2
tzdata==2025.2
uritemplate==4.1.1
urllib3==1.26.20
watchdog==6.0.0
Werkzeug==3.1.3
wheel==0.45.1
yarl==1.19.0






# metrics.py

# metrics.py
import numpy as np

def mean_absolute_error(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

def root_mean_squared_error(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mean_absolute_percentage_error(y_true, y_pred):
    # Avoid division by zero
    y_true = np.where(y_true == 0, 1e-8, y_true)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100





#auth.py

# auth.py
import streamlit as st

# Dummy user store (replace with Firebase later)
USERS = {
    "admin": {"password": "procodeadmin@123", "name": "Admin"},
}

def login_form():
    st.subheader("🔐 Login with Username & Password")
    username = st.text_input("Username")
    password = st.text_input("Password", type="password")
    if st.button("Login"):
        if username in USERS and USERS[username]["password"] == password:
            st.session_state["user"] = USERS[username]["name"]
            st.success(f"Welcome {USERS[username]['name']} 🎉")
            return True
        else:
            st.error("Invalid username or password")
            return False
    return False






#firebase_auth.py


# firebase_auth.py
import streamlit as st
import pyrebase

firebaseConfig = {
    "apiKey": "YAIzaSyAO0O5rV6qBAH2rvKMcrJyEPofa5kRZKzoOUR_API_KEY",
    "authDomain": "procode-modeling-platform.firebaseapp.com",
    "projectId": "procode-modeling-platform",
    "storageBucket": "procode-modeling-platform.firebasestorage.app",
    "messagingSenderId": "557435622129",
    "appId": "1:557435622129:web:a2971130b34f3189ea0f50",
    "measurementId": "G-BEMNMY4JL6",
    # "databaseURL": ""
}

firebase = pyrebase.initialize_app(firebaseConfig)
auth = firebase.auth()

def email_password_login():
    st.subheader("📧 Login with Email & Password")
    email = st.text_input("Email")
    password = st.text_input("Password", type="password")
    if st.button("Login with Email"):
        try:
            user = auth.sign_in_with_email_and_password(email, password)
            st.session_state["user"] = email
            st.success(f"✅ Logged in as {email}")
            return True
        except Exception as e:
            st.error("❌ Invalid email or password")
            return False
    return False


def google_login():
    st.subheader("🔑 Login with Google")
    st.markdown(
        "[Click here to sign in with Google](https://accounts.google.com/o/oauth2/v2/auth)"
    )
    st.info("👉 For now, you can test Google login directly via Firebase console. "
            "Advanced integration with OAuth flow can be added later.")






